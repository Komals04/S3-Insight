#!/usr/bin/env python3

import csv
import datetime
import boto3

def get_bucket_names(s3_client):
    """Returns the name of every bucket in this S3 account."""
    resp = s3_client.list_buckets()
    return [bucket["Name"] for bucket in resp["Buckets"]]

def get_size_of_bucket(s3_client, cloudwatch_client, *, bucket_name):
    """
    Given the name of a bucket, return a rough estimate for the bytes in this
    bucket per storage class.
    """
    # Existing code ...

    # Get the lifecycle configuration for the bucket
    try:
        lifecycle_config = s3_client.get_bucket_lifecycle_configuration(Bucket=bucket_name)
    except s3_client.exceptions.NoSuchLifecycleConfiguration:
        lifecycle_config = None

    rv["lifecycle_config"] = lifecycle_config

    return rv

# Modify the write_csv function to include the lifecycle configuration information
def write_csv(bucket_sizes):
    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    csv_name = f"s3_summary_spreadsheet_{now}.csv"

    storage_names = [
        "StandardStorage",
        "StandardIAStorage",
        "StandardIASizeOverhead",
        "ReducedRedundancyStorage",
        "GlacierStorage",
        "GlacierObjectOverhead",
        "GlacierS3ObjectOverhead",
        "DeepArchiveStorage",
        "DeepArchiveObjectOverhead",
        "DeepArchiveS3ObjectOverhead",
        "DeepArchiveStagingStorage",
    ]

    # Add lifecycle configuration fieldnames
    lifecycle_fieldnames = ["Rules"]

    fieldnames = ["bucket name", "number of objects"] + lifecycle_fieldnames

    for name in storage_names:
        fieldnames.append(f"{name} (bytes)")
        fieldnames.append(f"{name} (human-readable)")

    with open(csv_name, "w") as outfile:
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()

        for bucket in bucket_sizes:
            row = {
                "bucket name": bucket.pop("bucket name"),
                "number of objects": bucket.pop("number of objects"),
            }

            # Add lifecycle configuration data to the row
            lifecycle_config = bucket.pop("lifecycle_config", {})
            row["Rules"] = lifecycle_config.get("Rules", [])

            for storage_class, byte_count in bucket.items():
                row[f"{storage_class} (bytes)"] = byte_count
                row[f"{storage_class} (human-readable)"] = naturalsize(byte_count)

            writer.writerow(row)

    return csv_name

# Existing code ...

if __name__ == "__main__":
    s3_client = boto3.client("s3")
    cloudwatch_client = boto3.client("cloudwatch")

    bucket_names = get_bucket_names(s3_client)

    bucket_sizes = [
        get_size_of_bucket(s3_client, cloudwatch_client, bucket_name=name) for name in bucket_names
    ]

    csv_name = write_csv(bucket_sizes)
    print(f"✨ Written a summary of your S3 stats to {csv_name} ✨")
